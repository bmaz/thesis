\chapter{Detecting Twitter events}


\section{Introduction}
Twitter has been used to detect or predict a large variety of events,
from flood prevention \cite{de2017towards} to stock market movements
\cite{pagolu2016sentiment}. However, the specificity of social network data (short texts, use of
slang, abbreviations, hashtags, images and videos, very
high volume of data) makes all "general" detection tasks
(without specification
of the type of topic)
very difficult on tweet datasets.This is all the
more true for non-English languages.

Many works on event detection are actually
focused on burst detection (detecting topics such as
natural disasters, attacks, etc., that cause an unusual
volume of tweets), and do not attempt to assess the
relative size of events. We seek to detect \textit{all} events, both
those that generate a high volume of tweets and those that
are little discussed, and to group together \textit{all} tweets
related to the same event. With this definition in mind, the
topic detection and tracking task is conceptually similar to 
clustering. Given the size of our tweet collection, 
the chosen clustering method has to be extremely time-efficient.

Apart from the choice of the event detection algorithm, 
we also have to consider the type of tweet representation: 
should we only use the text of the tweets, or should we consider 
the tweet as a multimodal document, composed of text and/or images, videos, hashtags?

In the field of language processing, recent works
have made it possible to reach performance close to human capacity, 
particularly with regard to the evaluation of the semantic similarity 
between two sentences\footnote{See the results on the GLUE benchmark: \url{https://gluebenchmark.com/leaderboard}}. However, these advances, 
based on the training of neural networks on very large corpora of texts,
may not always be suited to our task. Indeed, despite rapid progress 
in recent years in the adaptability of
language processing (the GLUE benchmark \cite{wang2018glue} consists of
9 different tasks, and the models are evaluated according to their average performance on
all these tasks), it remains difficult to adapt these models to new tasks. 
Any transformation of the initial task requires fine-tuning a neural network 
on (at least) a few thousand sentences, which
involves hours of manual annotation to create a suitable dataset. 
In our work we focus on short-text topic similarity (evaluate if two sentences / short texts
address the same subject), which in some cases differs from semantic similarity
(evaluate whether two sentences mean the same thing) evaluated in GLUE, and we don't 
have a corresponding training dataset. Besides, even on a strictly identical
task, the performances announced in the litterature are perfectly reproducible 
only on English language corpora. Finally, most of these models are designed to be used as input to
end-to-end systems. For example, to calculate a similarity score between sentences
with BERT \cite{devlin2018bert}, it is necessary to treat each couple of sentences instead of each sentence. 
These architectures do not apply well to information retrieval systems that involve comparing
hundreds of thousands of sentences. For clustering or information retrieval tasks, 
it is more efficient to represent each sentence in a vector space where similar sentences
 are close (so-called \textit{embeddings}), and then apply conventional distance measurements 
 (cosine, euclidean distance, etc.). 
 
 \begin{figure*}
  \includegraphics[width=\textwidth]{figures/Moon_horizontal.png}
    \caption{Example of a topic (the July 2018 lunar eclipse) where multimedia contents provide a critical information for topic detection}
    \label{fig:moon}
\end{figure*}
 
 Regarding images, recent progress in the field of visual content description due to deep neural networks provides new ways of representing social media documents by including rich visual features. There are cases where image provides decisive information for event detection (see Fig. \ref{fig:moon}). However, dealing with image tweets often requires a contextual knowledge external to the document. Without this knowledge, image can become a source of error for event detection algorithms compared to text alone.
 
We deepen these questions throughout the chapter, starting with a state of the art, then presenting different tested algorithms. Finally, we present the results of our experiments, first on text, then using multimodal approaches.


\section{State of the art}

\subsection{Definitions}
			There is no consensus on the formal definition of the problem of event detection on Twitter, mainly because of the diversity of use-cases and scenarios for which event detection may be useful. \citet{sankaranarayanan_twitterstand:_2009} aim at providing a user interface displaying breaking news from tweets in the same way as a news wire. \citet{aiello_sensing_2013} design their tool for ``an expert of some domain [who] has to monitor specific topics or events being discussed in social media". \citet{zhang_triovecevent:_2017} give several applications of their event detector, such as sending alarms in case of imminent disaster or helping local governments to prevent social unrest. These few examples highlight the fact that all authors working on the subject do not perceive \textit{``event detection"} in the same way: their tools do not detect the same \textbf{type of events} (local events, conversations about specific topics, breaking news...), they do not have the same \textbf{inputs} (tweets from a range of selected sources, geo-tagged tweets, tweets containing a given keyword...) and \textbf{outputs} (all detected events, the top $k$ events depending on some relevance criteria, a list of tweets linked to each event, a list of keywords...), and are not designed to handle the same \textbf{volume of tweets}. We propose here an overview of the existing definitions.
			
Even though it is not focused on tweets but rather on traditional news contents, many articles on Twitter event detection refer to the definition given in the Topic Detection and Tracking project \citep{allan_introduction_2002}: an event is ``something that happens at a specific time and place along with all necessary conditions and unavoidable consequences". 
For \citet{aggarwal_event_2012} and \citet{mcminn_building_2013}, the definition of event is similar but includes another dimension: an event has to be ``of interest to the news media" \citep{aggarwal_event_2012}.
\citet{becker_beyond_2011} consider that events are necessarily linked to a real world fact: they define an event as ``a real-world occurrence $e$ with (1) an associated time-period $T_e$ and (2) a time-ordered stream of Twitter messages $M_e$, of substantial volume, discussing the occurrence and published during time $T_e$."  
 For \citet{hasan_survey_2018}, the notion of ``real world" is also mentioned. An event is ``an occurrence of interest in the real world which instigates a discussion on the event-associated topic by various users of social media, either soon after the occurrence or, sometimes, in anticipation of it." 
 In these definitions, the events have to exist outside social medias: they have to be either mentioned by other traditional media outlets, or be linked to an ``occurrence", a ``fact" in the real world.
 
Other authors define a social media event depending on the actions they generate on the studied social media. \citet{panagiotou_detecting_2016} call ``action" any activity on a social media, such as posting new content, interacting with another profile (sending a friend request, for example) or interacting with existing contents (like, or retweet). In their definition, an event is ``something that causes a large number of actions in the OSN [Online Social Network]". This type of approaches often consider events as ``breaking news" that will generate a larger activity on social media than any other topic at a given moment. This is also the approach of \citet{guille_event_2015}, that focus on detecting ``bursty" patterns.

For our study, we need to detect on Twitter any set of tweets from several sources that discuss the same fact. These facts do not have to exist outside of the social media (in the so called ``real world"). Besides, defining an event depending on its ``burstiness" can lead to neglect smaller size events.
		
		\subsection{Twitter event detection approaches}
		We divide the event detection methods into three types of approaches: term-weighting-based approaches, topic modeling, and incremental clustering. This is also the classification used in the survey on real-time event detection by \citet{hasan_survey_2018}. We will focus on the third approach, incremental clustering, since it is the one we chose to implement for our own event detection tool.
		
		\subsubsection{Term-weighting-based approaches}
		These approaches rely on tracking the terms likely to be linked to an event (often due to a high frequency of some terms during a given time window). They usually return a list of the top $k$ trending events on Twitter, which does not meet our objective of detecting events in an exhaustive manner.
		
		The event detection system TwitterMonitor \citep{mathioudakis_twittermonitor:_2010} detects bursty keywords in the Twitter stream by comparing their term frequencies in previous periods to current term frequency. Bursty keywords are then grouped together in ``trends" depending on their co-occurrences in recent tweets. 
		
		EnBlogue \citep{alvanaki_see_2012} measures the correlation of hashtags pairs within a given time window. Emergent topics are then detected among the pairs with the highest shift in their correlation. EnBlogue produces an overall scoring of the topics depending on the shift in correlation and on the total popularity of each topic. This score is smoothed in order to give a higher rank to new topics. 
		
		MABED \citep{guille_event_2015} does not only uses the textual content of the tweets: the frequency at which users interact with each other using ``mentions" (i.e. the name of another Twitter account preceded by ``@") is also taken into account to detect events. The system models the number of tweets that contain word $t$ and at least one mention during a given time-window as a binomial distribution. It detects positive anomalies if the creation of mentions associated to word $t$ is strictly greater than the expectation of the model. The magnitude of impact of an event on a time interval I is computed by integrating the anomaly function on the interval I. For each word associated with a mention, the system finds the interval I that maximizes its magnitude of impact. After other steps of event description and duplicates removal, the events with the highest impact are returned.
		
		The Twitter Live Detection Framework (TLDF) \citep{gaglio_framework_2016} modifies the Soft Frequent Pattern Mining (SFPM) algorithm \citep{petkos_soft_2014} to adapt to the dynamic nature of tweets. The authors use the relevance score of term $t$ proposed for SFPM: the ratio of the likelihood of appearance of the term in the current time-window and in a reference set of tweets. This relevance score is combined with a parameter boosting the score of named entities and multiplied by the $\mbox{tf-idf}$ of term $t$. Moreover, the size of the detection time-window is not fixed, but is controlled by a sigmoid function depending on the volume of emitted tweets at a given moment.
				
		\subsubsection{Topic models} Topic models are widely used techniques in the natural language processing field to discover the topical structure from a corpus of textual documents (news articles, scientific papers, tweets, etc.). Latent Dirichlet Allocation (LDA) is the most common one \citep{blei_latent_2003}. In this model, each document is considered as a mixture of different topics drawn from a topic distribution. 
	
	In our case, LDA has several drawbacks: 1. the number of topics needs to be known in advance, 2. the model doesn't dynamically adapt to new documents joining the corpus, 3. it assumes that a document is a mixture of several topics, which is very rare in short texts like tweets.
	
	\begin{enumerate}
	 \item Regarding the number of topics, some methods exist to estimate the optimal parameter $K$ before performing LDA \citep{brunet_metagenes_2004,arun_finding_2010,greene_how_2014}. However this would require to re-run these estimations each time new documents have to be added to the existing clusters.
	 \item \cite{blei_dynamic_2006} have addressed the issue of topics evolving over time; however they assume that the number of topics remains the same over periods, whereas in a stream of tweets, topics can emerge and disappear at each new period.
	 \item To adapt LDA to short texts, several approaches have been proposed in past works: one of them, known as Dirichlet Multinomial Mixture (DMM) model is to restrict the document-topic distribution, such that each tweet is assumed to be generated from a single topic \citep{yin_dirichlet_2014}. However, this is a strong assumption, especially since the shift from 140-character to 280-character tweets in November 2017. \citet{li_enhancing_2017} propose to extend the DMM model by allowing short texts to be generated by one or more topics, where the topic number is reduced (from 1 to 3) and drawn from a Poisson distribution.
	\end{enumerate}

	\subsubsection{Incremental clustering \label{Subsubsec: incremental clustering}} Incremental clustering (also called online clustering) do not require a fixed number of clusters. These methods are well fitted for discovering clusters of textual documents dynamically as new documents are added to the collection.  This type of approaches generally use the state of the art First Story Detection (FSD) \citep{allan_introduction_2002} as reference method: documents are represented in a vector space using $\mbox{tf-idf}$, their similarity is computed using cosine similarity, and each new document joins the cluster of its nearest neighbor in the collection. If the distance to the nearest neighbor is higher than a pre-defined threshold $d$ a new cluster is created, that contains only the new document. Regularly, the oldest documents (that were created before $t_0 - t$ with $t_0$ being the current time and $t$ a fixed parameter) are dropped from the collection. This insures that the publication time of documents is taken into account, which is also an advantage of incremental clustering compared to other clustering methods, that may cluster together documents emitted at very different periods.

Existing works on tweet incremental clustering have adapted this reference method initially developed for streams of news (such as RSS streams) to process much higher volumes of documents, and very short texts. Indeed, hundreds of tweets per second are collected using the public Twitter APIs, and the maximum size of a tweet is 280 characters (140 characters before 2018). Changes in that baseline method are done either in the type of text representation, or in the clustering algorithm itself. As step of noise filtering is also frequently added before the clustering step, to distinguish event from non-event tweets (according to \citet{liu_reuters_2016}, the proportion of event tweets in well established corpora such as the one by \citet{mcminn_building_2013} is less than 0.2\%). This section explores the variations introduced in the clustering algorithm.


In TwitterStand, \citet{sankaranarayanan_twitterstand:_2009} first perform a filtering step, that classify tweets as either ``junk" or ``news" using a naive Bayes classifier. Then, their online clustering algorithm associates a vector to each cluster, which is composed from the contained tweets' terms weighted with tf-idf. Each new tweet is represented using tf-idf and compared to the clusters' vectors using a modified cosine distance that accounts for the temporal dimension of clusters. The distance formula is 
$$
\dot{\delta}(t,c) = \delta(t,c)\times e^{\frac{-(T_t-T_c)^2}{2\sigma^2}}
$$

where $\delta(t,c)$ is the cosine distance between tweet $t$ and cluster $c$, $T_t$ is tweet $t$'s publication time and $T_c$ is cluster $c$'s mean publication time. The distance is only computed for clusters with a mean publication time $T_c$ more recent than 3 days, and that have a word in common with $t$. If $\dot{\delta}(t,c^*) \leqslant \epsilon$ (where $c^*$ is the nearest cluster to $t$), the tweet $t$ is added to the cluster $c^*$. Else, a new cluster is created. To speed up the search for the nearest cluster, an inverted index of the cluster's words is maintained: for each word $w$, the inverted index stores pointers to the clusters that contain $w$. The clustering algorithm in TwitterStand is adapted to the noisy nature of tweets by maintaining a list of reputable sources. A cluster is dropped from the list of active clusters if none of the first $k$ tweets are from a reputable source. The system also deals with fragmentation (the fact that several clusters about the same topic are created) by removing duplicate clusters. If a cluster $c'$  is identified as the duplicate of an older cluster $c$, $c'$ is marked as ``slave" of the cluster $c$, and $c$ as ``master" of $c'$. Any tweet that should be added to $c'$ is now added to $c$.

\citet{petrovic_streaming_2010} speed up the standard FSD algorithm by replacing the nearest neighbor search by an approximate nearest neighbor search using Locality Sensitive Hashing (see appendix \ref{Appendix: LSH} for a detailed presentation of LSH). Instead of searching for the nearest neighbor in the set of all documents, the search is done among a small set $S$ of potential nearest neighbors. The set $S$ is the set of documents that have the same hash as current document $x$:
$$
S(x) = \{y: h_{ij}(y) = h_{ij}(x), \exists i \in [1 \ldots L], \forall j \in [1 \ldots k]\}
$$
where $L$ is the number of hash tables, and $k$ the number of hyperplanes in each hash table. The hash functions $h_{ij}$ are defined as:
$$
h_{ij}(x) = sgn(u_{ij}^Tx)
$$
the sign of the scalar product of random vector $u_{ij}$ and $x$. To reduce the risk of failing to find the nearest neighbor (if the nearest neighbor $y$ does not collide with $x$ in the set $S(x)$), if no nearest neighbor is found, the system starts a search of exact nearest neighbor through an inverted index containing only most recent documents. To limit the growth of the sets $S$ (also called buckets), the number of documents in a single bucket is limited to a constant parameter. The oldest documents in the bucket are then removed. 

The system proposed by \citet{petrovic_streaming_2010} has later been improved  \citep{petrovic_using_2012} by using public synonym and paraphrase corpora in order to expand the words of the tweets with all their potential synonyms. This approach is a way to compensate for the scarcity of information contained in tweets and to increase the chances of finding a potential neighbor even if there is no tweet containing the same words in the collection.

\citet{becker_beyond_2011} use the same clustering algorithm as \citet{sankaranarayanan_twitterstand:_2009}, with each new tweet being represented as a tf-idf vector and compared using cosine similarity to the centroid of each cluster (the centroid is the mean weight of all terms in all tweets contained in the cluster). The specificity of their approach lies in the step following the clustering: instead of processing to a noise/event classification at the tweet level, the classification step takes place at the cluster level. They use a wide range of features in order to run a classification algorithm able to distinguish event clusters from noise clusters. The considered features are temporal features (taking into account the frequency of emission of the tweets in the cluster), social features (percentage of messages being retweets, replies, or mentions), topical features (based on the assumption that event clusters have a smaller diversity of topics), and Twitter-Centric features (hashtag usage, presence of multi-word hashtags).

\citet{mcminn_real_2015} base their event-detection approach on Part Of Speech (POS) Tagging and Named Entity Recognition (NER). Their first step is to extract named entities (persons, locations and organizations) and lemmatized nouns and verbs from each tweets. They then proceed to an aggressive filtering step (95\% of the tweets are removed): the system removes retweets, tweets with no named entities, and tweets containing terms associated with noise (``follow", ``watch", etc.). Then two steps are conducted in parallel: a clustering and a burst detection step. The clustering is based on an inverted index of all named entities: for each new tweet, the tweets containing the same named entities are retrieved from the index. The nearest neighbor is searched among these tweets using tf-idf representation, cosine similarity and a similarity threshold. If a nearest neighbor is found and does not already belong to a cluster, a new cluster is created containing the two tweets. In parallel, the bust detection module looks for bursts in the frequency of the detected entities. Once a burst has been detected, the clusters associated with the given entity are associated to it if the average timestamp of the tweets in the cluster is after the initial burst. The clusters associated to one burst form an event. If an entity associated to another event is mentioned in more than 50\% of the event's tweets, the two events are merged.

With Reuters Tracer, \citet{liu_reuters_2016} propose an event detection system that is also largely based on POS tagging and NER. This system combines a tweet level noise filtering and a cluster level noise filtering. Are considered as ``noise" all tweets and all clusters of tweets that are not linked to an event (in the sense of \citet{mcminn_building_2013}). The clustering algorithm itself is quite different from the standard FSD algorithm. New tweets are added to clusters based on three criteria: (1) Retweets of the same tweets are all added to the same cluster. (2) Tweets containing the same url are all added to the same cluster. (3) Finally, a similarity metric with the existing clusters is computed as follows:
$$
S_i = aN_e + bN_n + cN_v + dN_h
$$
where $N_e,N_n,N_v,N_h$ are the numbers of matching Named Entities, nouns, verbs and hashtags between the tweet and the cluster, and $a, b, c, d$ are learned parameters. Liu and al. explain that their system is primarily based on named entities (``An event is usually defined by \textit{who}, \textit{where} and \textit{what}" according to \citet{mcminn_building_2013}), and that there is no need for high dimensional tf-idf vectors to compute a good similarity function. The obtained clusters, called ``unit clusters" are progressively merged into bigger clusters using the same steps (merging based on retweets, urls, then similarity metric) as the unit clustering step. However, the used $a, b, c, d$ parameters are different.

\citet{hasan_twitternews_2016} use the technique of First Story Detection with LSH developed by \citet{petrovic_streaming_2010}. However, they observe that the $k$ independent random vectors in a single hash table need to be updated every time the size of the input tf-idf vectors increases (which happens at a high rate in Twitter data since the vocabulary evolves faster than in traditional news). To alleviate this problem, they propose a technique using random indexing \citep{sahlgren_introduction_2005}. Random indexing  aims at representing terms with fixed-size vectors. Each term $t$ is associated with two vectors: an index vector and a context vector. The index vector is a random vector. The context vector has the same size as the index vector and it is initialized with zeroes. When a co-occurrence of the term $t$ with another term $t'$ is observed, the context vector of $t$ is updated by adding the index vector of $t'$. A tweet can then be represented as an average of the vectors of each term in the tweet. This tweet representation is then used to perform the LSH approximate neighbors search of the tweet $d$. However, once the set $S$ of all documents that collide with $d$ in a hash table is computed, the exact nearest neighbor search is done using cosine similarity on tf-idf vectors, rather than random indexing vectors. \citet{hasan_twitternews_2016} use this method as a first algorithm in order to detect ``non unique" tweets, that are then clustered using a second algorithm close to the one by \citet{sankaranarayanan_twitterstand:_2009}: this time, the similarity to the clusters' centroids is computed. Hasan et al. explain the use of this two-steps clustering method by the necessity to restrain the number of ``one tweet clusters". A defragmentation step is finally done in order to merge similar clusters.


In these works, tweets are represented in the form of tf-idf vectors in the vast majority of cases \citep{sankaranarayanan_twitterstand:_2009, petrovic_streaming_2010, becker_automatic_2011, hasan_twitternews_2016}. \citet{repp_extracting_2018} test different types of representation of tweets (Word2Vec average, GloVe average, Doc2Vec, Word2Vec average weighted by the idf of each word). However, these representations are only tested on a classification task, and the best representation (the average of Word2Vec) is then used for clustering. We therefore wanted to update this work by testing recent embeddings, and in particular those developed for representation of sentences.

\subsection{Text embeddings}
The most commonly used "vectorization" method until the 2010s
was the tf-idf, introduced by \citet{sparck_1972_statistical}. This is an improvement to the principle “bag of words” vectors \cite{harris1954distributional}, where each document is described by the
number of occurrences of the words it contains ("term frequency"). The tf-idf use the same vectors, but weights each of the words in inverse proportion of the number of documents in which it appears.

\section{Algorithms}

\section{Text-only approaches}

\section{Multimodal approaches}